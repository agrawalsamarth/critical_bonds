import numpy as np
from numpy.polynomial.laguerre import lagval
from sklearn.linear_model import Ridge


# --------------------------- Feature map ---------------------------

def laguerre_features_pRB(p, R, B, t_scaled=0.0, degree=4, cross_PR=True):
    """
    Laguerre basis in price p and storage R (with p×R cross terms),
    plus a light basis for budget B, and a time scalar feature t/T.
    Returns X with shape (n, n_features).

    - p, R, B: arrays of same length
    - t_scaled: float in [0,1]
    """
    p = np.asarray(p).reshape(-1)
    R = np.asarray(R).reshape(-1)
    B = np.asarray(B).reshape(-1)

    # Scale to ~[0,1] for numerical stability
    p_s = p / (np.max(p) + 1e-9)
    R_s = R / (np.max(R) + 1e-9)
    B_s = B / (np.max(B) + 1e-9)

    I = np.eye(degree + 1)
    Lp = np.column_stack([lagval(p_s, I[d]) for d in range(degree + 1)])   # (n, d+1)
    LR = np.column_stack([lagval(R_s, I[d]) for d in range(degree + 1)])   # (n, d+1)

    # Budget: keep light (degree 2) to avoid blowing up feature size
    I2 = np.eye(3)
    LB = np.column_stack([lagval(B_s, I2[d]) for d in range(3)])           # (n, 3)

    feats = [Lp, LR, LB]
    if cross_PR:
        # Cross p-R excluding constants
        PR = []
        for i in range(1, degree + 1):
            for j in range(1, degree + 1):
                PR.append(Lp[:, i] * LR[:, j])
        feats.append(np.column_stack(PR))

    # Time scalar
    time_col = np.full((len(p), 1), float(t_scaled))
    feats.append(time_col)

    return np.hstack(feats)  # (n, n_features)


# --------------------------- Scenario generator ---------------------------

def generate_scenarios(T, M, rng, base_curve, sigma=4.0, mean_revert=0.4):
    """Mean-reverting Gaussian deviations around base_curve."""
    P = np.zeros((T, M))
    for m in range(M):
        x = base_curve[0] + rng.normal(0, sigma)
        P[0, m] = max(0.0, x)
        for t in range(1, T):
            # OU-like pull toward base
            x += mean_revert * (base_curve[t] - x) + rng.normal(0, sigma)
            P[t, m] = max(0.0, x)
    return P


# --------------------------- LSMC (Fitted Value / Policy Iteration) ---------------------------

class LSMC_Battery:
    """
    Robust LSMC for storage with state (p, R, B, t) using pathwise fitted value iteration.

    - We learn V_t(p, R, B) via Ridge on a Laguerre feature map.
    - Backward passes minimize squared error to Bellman targets computed with
      a discrete action set and next-step learned V_{t+1}.
    - We iterate (policy iteration style) to sharpen the buy/sell boundary.
    """

    def __init__(self, T, R_max, eta_c, c_max, d_max, cycles,
                 day_len=24, NA=21, degree=4, alpha=1e-3, iters=2, eps_explore=0.5):
        self.T = int(T)
        self.R_max = float(R_max)
        self.eta_c = float(eta_c)
        self.c_max = float(c_max)
        self.d_max = float(d_max)
        self.cycles = int(cycles)
        self.day_len = int(day_len)

        self.degree = int(degree)     # Laguerre degree for p & R
        self.alpha = float(alpha)     # Ridge strength
        self.iters = int(iters)       # number of value-iteration sweeps
        self.eps_explore = float(eps_explore)  # epsilon for exploration policy

        # Discrete action grid: negative=discharge, positive=charge
        self.action_grid = np.linspace(-self.d_max, self.c_max, NA)

        # Value models per time t (Ridge)
        self.models = [None] * (self.T + 1)  # include terminal slot for convenience

    def daily_buy_cap(self):
        return self.cycles * self.R_max / max(self.eta_c, 1e-12)

    # ---------- feasibility and transition ----------
    def caps(self, R, B):
        cap_buy  = min(self.c_max, B, (self.R_max - R) / max(self.eta_c, 1e-12))
        cap_sell = min(self.d_max, R)
        return cap_buy, cap_sell

    def step_transition(self, R, B, a):
        """Apply action a to (R,B) with efficiency on charge, return (R', B')."""
        a_pos = max(a, 0.0)
        a_neg = max(-a, 0.0)
        Rn = np.clip(R + self.eta_c * a_pos - a_neg, 0.0, self.R_max)
        Bn = max(0.0, B - a_pos)
        return Rn, Bn

    # ---------- V prediction ----------
    def V_next(self, t1, p1, R1, B1):
        """Predict V_{t1}(p1,R1,B1) using model at time t1 (or salvage if t1==T)."""
        if t1 >= self.T or self.models[t1] is None:
            # Terminal salvage
            return p1 * R1
        X = laguerre_features_pRB(p1, R1, B1, t_scaled=t1 / self.T,
                                  degree=self.degree, cross_PR=True)
        return self.models[t1].predict(X)

    # ---------- Build a pathwise state cloud with exploration ----------
    def _rollout_states(self, prices, seed=0):
        """
        Create a state cloud (p_t, R_t, B_t) for all t,m by simulating with an eps-greedy policy
        using current models (or random if not trained yet).
        """
        rng = np.random.default_rng(seed)
        T, M = prices.shape
        p_mat = prices
        R_mat = np.zeros((T, M))
        B_mat = np.zeros((T, M))
        R = np.zeros(M)
        B = np.full(M, self.daily_buy_cap())

        for t in range(T):
            p_t = p_mat[t, :]
            a_chosen = np.zeros(M)

            for m in range(M):
                # epsilon exploration
                if (self.models[t] is None) or (rng.random() < self.eps_explore):
                    # random feasible action
                    cap_buy, cap_sell = self.caps(R[m], B[m])
                    # sample from action grid until feasible
                    for _ in range(10):
                        a = rng.choice(self.action_grid)
                        if (a > 0 and a <= cap_buy) or (a <= 0 and -a <= cap_sell):
                            a_chosen[m] = a
                            break
                    else:
                        a_chosen[m] = 0.0
                else:
                    # greedy wrt current models: 1-step lookahead
                    best_val = -1e18
                    best_a = 0.0
                    cap_buy, cap_sell = self.caps(R[m], B[m])
                    for a in self.action_grid:
                        if a > 0 and a > cap_buy:   continue
                        if a < 0 and -a > cap_sell: continue
                        Rn, Bn = self.step_transition(R[m], B[m], a)
                        p1 = p_mat[min(t + 1, T - 1), m]
                        # reward now + continuation next
                        reward = p_t[m] * (max(-a, 0.0) - max(a, 0.0))
                        cont = self.V_next(t + 1, np.array([p1]), np.array([Rn]), np.array([Bn]))[0]
                        val = reward + cont
                        if val > best_val:
                            best_val = val
                            best_a = a
                    a_chosen[m] = best_a

                # apply transition
                R[m], B[m] = self.step_transition(R[m], B[m], a_chosen[m])

            # write state snapshot
            R_mat[t, :] = R
            B_mat[t, :] = B

            # daily reset of budget
            if (t + 1) % self.day_len == 0:
                B[:] = self.daily_buy_cap()

        return p_mat, R_mat, B_mat

    # ---------- One full backward sweep (FVI Bellman regression) ----------
    def _backward_fit(self, prices, p_mat, R_mat, B_mat):
        """
        Fit V_t(p,R,B) for all t by regressing Bellman targets using pathwise states.
        """
        T, M = prices.shape

        # Terminal model: V_T(p,R,B) = p_T * R (we emulate via None and handle in V_next)
        self.models[self.T] = None

        for t in reversed(range(T)):
            p_t = p_mat[t, :]
            R_t = R_mat[t, :]
            B_t = B_mat[t, :]

            # compute Bellman targets: max_a [ reward + V_{t+1}(...) ]
            y = np.empty(M, dtype=float)
            for m in range(M):
                p = p_t[m]
                R = R_t[m]
                B = B_t[m]

                cap_buy, cap_sell = self.caps(R, B)
                best = -1e18
                for a in self.action_grid:
                    if a > 0 and a > cap_buy:   continue
                    if a < 0 and -a > cap_sell: continue
                    Rn, Bn = self.step_transition(R, B, a)
                    reward = p * (max(-a, 0.0) - max(a, 0.0))
                    p1 = prices[min(t + 1, T - 1), m]
                    cont = self.V_next(t + 1, np.array([p1]), np.array([Rn]), np.array([Bn]))[0]
                    best = max(best, reward + cont)
                y[m] = best

            # fit V_t(p,R,B) ~ y
            X = laguerre_features_pRB(p_t, R_t, B_t, t_scaled=t / self.T,
                                      degree=self.degree, cross_PR=True)
            model = Ridge(alpha=self.alpha, fit_intercept=False)
            model.fit(X, y)
            self.models[t] = model

    # ---------- Public fit with policy iteration ----------
    def fit(self, prices, seed=123):
        """
        In-sample training:
        1) Build exploratory state cloud with epsilon-greedy rollout under current models.
        2) Backward Bellman regression to fit V_t.
        3) Repeat for a few iterations (iters) to sharpen boundaries.
        """
        for _ in range(self.iters):
            p_mat, R_mat, B_mat = self._rollout_states(prices, seed=seed)
            self._backward_fit(prices, p_mat, R_mat, B_mat)
            seed += 1  # change rollout randomness across iterations

    # ---------- Greedy action under learned V ----------
    def decide(self, t, R, B, p, p_next):
        cap_buy, cap_sell = self.caps(R, B)
        best_val, best_a, best_Rn, best_Bn = -1e18, 0.0, R, B
        for a in self.action_grid:
            if a > 0 and a > cap_buy:   continue
            if a < 0 and -a > cap_sell: continue
            Rn, Bn = self.step_transition(R, B, a)
            reward = p * (max(-a, 0.0) - max(a, 0.0))
            cont = self.V_next(t + 1, np.array([p_next]), np.array([Rn]), np.array([Bn]))[0]
            val = reward + cont
            if val > best_val:
                best_val, best_a, best_Rn, best_Bn = val, a, Rn, Bn
        return best_a, best_Rn, best_Bn, best_val

    # ---------- Evaluate on a price matrix ----------
    def evaluate(self, prices):
        T, M = prices.shape
        profits = np.zeros(M)
        for m in range(M):
            R, B, total = 0.0, self.daily_buy_cap(), 0.0
            for t in range(T):
                p = prices[t, m]
                p_next = prices[min(t + 1, T - 1), m]
                a, R, B, _ = self.decide(t, R, B, p, p_next)
                total += p * (max(-a, 0.0) - max(a, 0.0))
                if (t + 1) % self.day_len == 0:
                    B = self.daily_buy_cap()
            # salvage
            total += prices[-1, m] * R
            profits[m] = total
        return float(np.mean(profits)), profits

    # ---------- Print deterministic schedule ----------
    def print_schedule(self, base_curve, start_R=0.0):
        T = len(base_curve)
        R, B = float(start_R), self.daily_buy_cap()
        total = 0.0
        total_buy = 0.0
        total_sell = 0.0
        print("\nPolicy schedule along deterministic path:")
        print("t   Price    Action    Storage    Budget")
        for t in range(T):
            p = float(base_curve[t])
            p_next = base_curve[min(t + 1, T - 1)]
            a, Rn, Bn, _ = self.decide(t, R, B, p, p_next)
            total += p * (max(-a, 0.0) - max(a, 0.0))
            total_buy  += max(a, 0.0)
            total_sell += max(-a, 0.0)
            print(f"{t:02d}  {p:6.2f}   {a:7.2f}   {Rn:8.2f}   {Bn:8.2f}")
            R, B = Rn, Bn
            if (t + 1) % self.day_len == 0:
                B = self.daily_buy_cap()
        total += base_curve[-1] * R
        if R > 1e-9:
            print(f"** Terminal salvage of {R:.2f} at price {base_curve[-1]:.2f} adds {base_curve[-1]*R:.2f}")
        print(f"\nTotals — Buy: {total_buy:.2f}, Sell: {total_sell:.2f}, Profit: {total:.2f}")


# --------------------------- Example run ---------------------------

if __name__ == "__main__":
    rng = np.random.default_rng(42)
    T = 24
    base = np.array([40,38,36,35,34,33,32,35,45,55,60,70,
                     65,58,52,48,46,44,43,42,41,40,39,38], float)

    # In-sample (stochastic), Out-of-sample (deterministic copies of base)
    prices_train = generate_scenarios(T, 3000, rng, base, sigma=4.0, mean_revert=0.4)
    prices_test  = np.tile(base.reshape(-1, 1), (1, 2000))

    model = LSMC_Battery(
        T=T, R_max=100, eta_c=0.87,
        c_max=50, d_max=50, cycles=2,
        day_len=24,
        NA=25,           # finer action grid helps
        degree=4,        # richer Laguerre
        alpha=1e-3,      # ridge
        iters=2,         # two value-iteration sweeps
        eps_explore=0.6  # encourage exploration on first pass
    )

    # Train and evaluate
    model.fit(prices_train)
    mean_in, _      = model.evaluate(prices_train)
    mean_out, profs = model.evaluate(prices_test)

    print(f"\nIn-sample mean profit:  {mean_in:.2f}")
    print(f"Out-of-sample mean:     {mean_out:.2f}")
    print(f"Out-of-sample std:      {np.std(profs):.2f}")

    # Deterministic schedule on the base curve
    model.print_schedule(base_curve=base)
